- Un Transformer; Es un modelo de inteligencia artificial que aprende a entender relaciones entre palabras, frases o datos en general, poniendo más atención en las partes importantes y menos en las irrelevantes.

En vez de leer todo en orden (como lo hacen redes antiguas), el Transformer mira todo al mismo tiempo y decide a qué partes prestar más atención.

Es como leer un párrafo y saber de un vistazo qué palabra conecta con cuál, sin tener que ir una por una.




- Transformer tipo Linformer; es un Transformer que resume la información antes de compararla, para ser mucho más rápido y eficiente con secuencias largas.




- Un token; Es una pequeña parte de un texto, como una palabra o un fragmento, que el modelo usa para entender o generar lenguaje.




- Dataset; Un dataset es el conjunto de datos que usa una IA para aprender




- Dataset simulado; Es un conjunto de datos creados artificialmente para entrenarla cuando no hay datos reales disponibles.




- Embedding es como un "valor numérico especial" que tiene cada token, y tokens parecidos tienen valores parecidos.




- Dimensión; La cantidad de números que usas para describir algo (como en los ejes cartesianos usas x, y; El cual son dos numeros, en una dimensión de 120 utilizarias 120 numeros).




- Redes neuronales recurrentes: Una red neuronal recurrente es una red que tiene memoria y sirve para entender secuencias, como si fuera leyendo palabra por palabra y recordando lo que ya leyó.




- red neuronal: Una red neuronal es un sistema que aprende a hacer tareas imitando cómo piensa el cerebro, usando muchas pequeñas unidades conectadas que procesan información juntas.




- PyTorch: Es una herramienta que permite crear y entrenar modelos de inteligencia artificial de forma fácil y flexible, usando principalmente el lenguaje de programación Python.




- Genoma: Un genoma es el conjunto completo de instrucciones que tiene un ser vivo para funcionar, como si fuera el "manual" que dice cómo está hecho y cómo trabaja.






- 




- 




- 




- 




- 




- 




- 




- 




- 




- 




- 




- 




- 




- 




- 




- 













